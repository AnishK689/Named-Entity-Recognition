{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Named Entity Recognition**"
      ],
      "metadata": {
        "id": "J17kJn85D9XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import the necessary NLTK libraries and download required data"
      ],
      "metadata": {
        "id": "ApyeBE_oCDrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6pZVnrOPCDoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3I_UyNi_yvF",
        "outputId": "39a497fd-a877-43ee-9fc0-928510c18b58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer\n",
        "from nltk import pos_tag, ne_chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define a function to extract and print named entities"
      ],
      "metadata": {
        "id": "VMwEHEjxCIJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_named_entities(text):\n",
        "    # Step 3: Tokenize the input text into sentences\n",
        "    sentences = PunktSentenceTokenizer(text).tokenize(text)\n",
        "\n",
        "    # Step 4: Loop through each sentence in the text\n",
        "    for sentence in sentences:\n",
        "       # Tokenize words in the sentence\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "       # Perform part-of-speech tagging on the words\n",
        "        tagged = pos_tag(words)\n",
        "\n",
        "       # Perform named entity chunking\n",
        "        named_entities = ne_chunk(tagged, binary=False)\n",
        "\n",
        "       # Step 5: Initialize variables to store named entity data\n",
        "        named_entity_types = set()\n",
        "        named_entity_dict = {}\n",
        "\n",
        "       # Step 6: Loop through the named entities and organize them by type\n",
        "        for chunk in named_entities:\n",
        "            if isinstance(chunk, nltk.Tree):\n",
        "                entity_type = chunk.label()\n",
        "                entity_name = ' '.join([token for token, tag in chunk.leaves()])\n",
        "                named_entity_types.add(entity_type)\n",
        "\n",
        "                if entity_type in named_entity_dict:\n",
        "                    named_entity_dict[entity_type].append(entity_name)\n",
        "                else:\n",
        "                    named_entity_dict[entity_type] = [entity_name]\n",
        "\n",
        "       # Step 7: Print the named entities by type\n",
        "        for entity_type in named_entity_types:\n",
        "            print(f'{entity_type} - {\", \".join(named_entity_dict[entity_type])}')"
      ],
      "metadata": {
        "id": "zqqo4fYbCPYn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Main program execution"
      ],
      "metadata": {
        "id": "uJ2ks38ZDagG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Step 9: Get user input for a sentence or paragraph\n",
        "    user_text = input(\"Enter a sentence or paragraph: \")\n",
        "\n",
        "    # Step 10: Call the get_named_entities function to extract and print named entities\n",
        "    get_named_entities(user_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUhLbVt9DeU6",
        "outputId": "7e91f1c6-494d-4e52-9c37-a6bb00ffc37b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence or paragraph: My name is Pooja. I am doing puja\n",
            "PERSON - Pooja\n"
          ]
        }
      ]
    }
  ]
}